---
title: "Module 11 Exercise (Analysis #3)"
author: "Morgan Taylor"
date: "11/5/2021"
output: html_document
---

# Module 11: Machine Learning Models 1

This RMD corresponds to the assignment in [Module 11 for MADA 2021](https://andreashandel.github.io/MADAcourse/Assessment_ML_Models_1.html). It loads the processed data and fits a three machine learning models after some data pre-processing.

<br>

## Libraries required:
* here: for data loading/saving
* tidyverse: for data management
* tidymodels: for data modeling
* skimr: for variable summaries
* broom.mixed: for converting bayesian models to tidy tibbles
* rpart.plot: for visualizing a decision tree
* vip: for variable importance plots
* glmnet: for lasso models
* doParallel: for parallel backend for tuning processes
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#libraries required
library(here) #for data loading/saving
library(tidyverse) #for data management
library(tidymodels) #for data modeling
library(skimr) #for variable summaries
library(broom.mixed) #for converting bayesian models to tidy tibbles
library(rpart.plot) #for visualizing a decision tree
library(vip) #for variable importance plots
library(glmnet) #for lasso methods
library(doParallel) #for parallel backend tuning

#set global environment to avoid scientific notation
options(scipen = 999)

#fix doParallel error
unregister_dopar <- function() {
     env <- foreach:::.foreachGlobals
     rm(list=ls(name=env), pos=env)
}
```

<br>

## Load Data
Load the processed data from the `processed_data` folder in the project file.
```{r load data}
#path to data
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","processeddata.rds")

#load data. 
data <- readRDS(data_location)

#summary of data using skimr package
skimr::skim(data)
```

<br>

## Pre-processing
There are two steps in this pre-processing: (1) feature removal and (2) addressing low ("near-zero") variance predictors.

**Feature Variable Removal**
In the output above, there are three variables that have both a severity score and a yes/no feature: weakness, cough, and myalgia. There are actually two variables for cough yes/no. These variables are strongly correlated and therefore affect model performance. Solution: remove all yes/no versions of variables for which a severity score exists.
```{r}
#variable names to remove: WeaknessYN, MyalgiaYN, CoughYN, CoughYN2
featadj_data <- dplyr::select(data, -c(WeaknessYN, MyalgiaYN, CoughYN, CoughYN2))
```

These severity scores are also ordered, so we need to specify the order: None < Mild < Moderate < Severe.
```{r}
#myalgia
featadj_data$Myalgia <- ordered(featadj_data$Myalgia, labels = c("None", "Mild", "Moderate", "Severe"))

#weakness
featadj_data$Weakness <- ordered(featadj_data$Weakness, labels = c("None", "Mild", "Moderate", "Severe"))

#cough
featadj_data$CoughIntensity <- ordered(featadj_data$CoughIntensity, labels = c("None", "Mild", "Moderate", "Severe"))

#double check to confirm code worked
skimr::skim(featadj_data)
```

<br>

**Low ("near-zero") variance predictors**
The skimr output shows there are some predictors that are fairly unbalanced with most patients reporting `no` and only a few `yes`. This can be handled automatically in `tidymodels` with `step_nzv()`, but it can be better to do it manually to ensure scientific relevance. Here, we will remove binary predictors that have <50 entries in one category. According to the `skimr::skim` output, there are two: `Hearing` and `Vision`.
```{r}
#drop Hearing and Vision from the dataset to create processed dataset for ML analysis
ML_processed <- dplyr::select(featadj_data, -c(Hearing, Vision))
```

<br>

## Analysis Code
Eventually we will rearrange the analysis files and scripts, but for the purposes of documentation, this is the analysis code for this exercise.

Here, we are focusing on a single outcome: BodyTemp (continuous). Therefore, these will be regression models, so we can compare using RMSE.

<br>

**Data Setup**
Following the parameters determined in the assignment guidelines:
* Set the random seed to `123`
* Split the dataset into 70% training, 30% testing with `BodyTemp` as stratification
* 5-fold cross validation, 5 times repeated, stratified on `BodyTemp` for the CV folds
* Create a recipe for data and fitting that codes categorical variables as dummy variables
```{r}
#set random seed to 123
set.seed(123)

#split dataset into 70% training, 30% testing
#use BodyTemp as stratification
data_split <- rsample::initial_split(ML_processed, prop = 7/10,
                                     strata = BodyTemp)

#create dataframes for the two sets:
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)

#training set proportions by BodyTemp
train_data %>%
  dplyr::count(BodyTemp) %>%
  dplyr::mutate(prop = n / sum(n))

#testing set proportions by BodyTemp
test_data %>%
  dplyr::count(BodyTemp) %>%
  dplyr::mutate(prop = n / sum(n))

#5-fold cross validation, 5 times repeated, stratified on `BodyTemp` for the CV folds
folds <- rsample::vfold_cv(train_data,
                           v = 5,
                           repeats = 5,
                           strata = BodyTemp)
  
#create recipe that codes categorical variables as dummy variables
flu_rec <- recipes::recipe(BodyTemp ~ ., data = train_data) %>%
           recipes::step_dummy(all_nominal_predictors())
```

<br>

**Null model performance**
Determine the performance of a null model (i.e. one with no predictors). For a continuous outcome and RMSE as the metric, a null model is one that predicts the mean of the outcome. Compute the RMSE for both training and test data for such a model.
```{r}
#create null model
null_mod <- parsnip::null_model() %>%
            parsnip::set_engine("parsnip") %>%
            parsnip::set_mode("regression")

#add recipe and model into workflow
null_wflow <- workflows::workflow() %>%
              workflows::add_recipe(flu_rec) %>%
              workflows::add_model(null_mod)

#"fit" model to training data
null_train <- null_wflow %>%
                parsnip::fit(data = train_data)

#summary of null model with training data to get mean (which in this case is the RMSE)
null_train_sum <- broom.mixed::tidy(null_train)
null_train_sum

#"fit" model to test data
null_test <- null_wflow %>%
                parsnip::fit(data = test_data)

#summary of null model with test data to get mean (which in this case is the RMSE)
null_test_sum <- broom.mixed::tidy(null_test)
null_test_sum

#RMSE for training data
null_RMSE_train <- tibble::tibble(
                      estimate = rmse_vec(truth = train_data$BodyTemp,
                                          estimate = rep(mean(train_data$BodyTemp), nrow(train_data))),
                      SE = " ",
                      model = "Null - Train")

#RMSE for testing data
null_RMSE_test <- tibble::tibble(
                      estimate = rmse_vec(truth = test_data$BodyTemp,
                                          estimate = rep(mean(test_data$BodyTemp), nrow(test_data))),
                      SE = " ",
                      model = "Null - Test")

```

**Tree Model**
Most of the code for this section comes from the [TidyModels Tutorial for Tuning](https://www.tidymodels.org/start/tuning/).
*1. Model Specification*
```{r}
#run parallels to determine number of cores
cores <- parallel::detectCores() - 1
cores

cl <- makeCluster(cores)

registerDoParallel(cl)

#define the tree model
tree_mod <-
  parsnip::decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  parsnip::set_engine("rpart") %>%
  parsnip::set_mode("regression")

#use the recipe specified earlier (line 133)
```

*2. Workflow Definition*
```{r}
#define workflow for tree
tree_wflow <- workflows::workflow() %>%
               workflows::add_model(tree_mod) %>%
               workflows::add_recipe(flu_rec)
```

*3. Tuning Grid Specification*
```{r}
#tuning grid specification
tree_grid <- dials::grid_regular(cost_complexity(),
                                 tree_depth(),
                                 min_n(),
                                 levels = 10)

#tree depth
tree_grid %>%
  dplyr::count(tree_depth)
```

*4. Tuning Using Cross-Validation and the `tune_grid()` function*
```{r}
#tune the model with previously specified cross-validation and RMSE as target metric
tree_res <- tree_wflow %>%
                tune::tune_grid(resamples = folds,
                                grid = tree_grid,
                                control = control_grid(verbose = TRUE),
                                metrics = yardstick::metric_set(rmse))

#collect metrics
tree_res %>% workflowsets::collect_metrics()

#default visualization
tree_res %>% autoplot()

#more detailed plot
tree_res %>%
  workflowsets::collect_metrics() %>%
  dplyr::mutate(tree_depth = factor(tree_depth)) %>%
  ggplot2::ggplot(aes(cost_complexity, mean, color = tree_depth)) +
           geom_line(size = 1.5, alpha = 0.6) +
           geom_point(size = 2) +
           facet_wrap(~ .metric, scales = "free", nrow = 2) +
           scale_x_log10(labels = scales::label_number()) +
           scale_color_viridis_d(option = "plasma", begin = 0.9, end = 0)

#turn off parallel cluster
stopCluster(cl)
doParallel::stopImplicitCluster()
unregister_dopar()

```

*5. Identify Best Model*
```{r}
#select the tree model with the lowest rmse
tree_lowest_rmse <- tree_res %>%
                        tune::select_best("rmse")

#finalize the workflow by using the selected lasso model
best_tree_wflow <- tree_wflow %>%
                      tune::finalize_workflow(tree_lowest_rmse)
best_tree_wflow

#one last fit on the training data
best_tree_fit <- best_tree_wflow %>%
                    parsnip::fit(data = train_data)

#create a table of model fit
tree_tibble <- best_tree_fit %>%
                    workflowsets::extract_fit_parsnip() %>%
                    broom::tidy() %>%
                    dplyr::mutate_if(is.numeric, round, 4)
tree_tibble


```

*6. Model evaluation*
```{r}
#plot the tree
rpart.plot::rpart.plot(x = workflowsets::extract_fit_parsnip(best_tree_fit)$fit,
                       roundint = F,
                       type = 5,
                       digits = 5,
                       main = "Selected Tree Model")

#find predictions and intervals
tree_resid <- best_tree_fit %>%
                  broom.mixed::augment(new_data = train_data) %>%
                  dplyr::select(.pred, BodyTemp) %>%
                  dplyr::mutate(.resid = BodyTemp - .pred)

#plot model predictions from tuned model versus actual outcomes
#now use ggplot
ggplot2::ggplot(tree_resid, aes(x = BodyTemp, y = .pred)) +
  geom_abline(slope = 1, intercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "Decision Tree Fit: Predicted vs. Actual Body Temperature",
        x = "Actual Body Temperature (F)",
        y = "Predicted Body Temperature (F)")

#plot model with residuals
ggplot2::ggplot(tree_resid, aes(x = .pred, y = .resid))+
  geom_hline(yintercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "Decision Tree Fit: Residuals vs. Fitted Body Temperature",
        x = "Fitted Body Temperature (F)",
        y = "Residual")

#print model performance
tree_res %>%
  tune::show_best(n = 10) %>%
  dplyr::select(rmse = mean, std_err, penalty) %>%
  dplyr::mutate(rmse = round(rmse, 3),
                std_err = round(std_err, 4),
                cost_complexity = scales::scientific(cost_complexity))
#this shows the 10 best performing hyperparameter sets

#compare model performance to null model
tree_RMSE <- lasso_res %>%
                tune::show_best(n = 1) %>%
                dplyr::transmute(
                  rmse = round(mean, 3),
                  SE = round(std_err, 4),
                  model = "LASSO") %>%
               dplyr::bind_rows(null_RMSE_train)
```

<br>

**LASSO Model**
Most of the code for this section comes from the [TidyModels Tutorial Case Study](https://www.tidymodels.org/start/case-study/).
*1. Model Specification*
```{r}
#define the lassso model
lasso_mod <-
  parsnip::linear_reg(mode = "regression",
                      penalty = tune(), 
                      mixture = 1) %>%
  parsnip::set_engine("glmnet")

#use the recipe specified earlier (line 133)
```

*2. Workflow Definition*
```{r}
#define workflow for lasso regression
lasso_wflow <- workflows::workflow() %>%
               workflows::add_model(lasso_mod) %>%
               workflows::add_recipe(flu_rec)
```

*3. Tuning Grid Specification*
```{r}
#tuning grid specification
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#5 lowest penalty values
lasso_grid %>%
  dplyr::top_n(-5)

#5 highest penalty values
lasso_grid %>%
  dplyr::top_n(-5)
```

*4. Tuning Using Cross-Validation and the `tune_grid()` function*
```{r}
#tune the model with previously specified cross-validation and RMSE as target metric
lasso_res <- lasso_wflow %>%
                tune::tune_grid(resamples = folds,
                                grid = lasso_grid,
                                control = control_grid(verbose = TRUE, save_pred = TRUE),                                    metrics = metric_set(rmse))

#look at 15 models with lowest RMSEs
top_lasso_models <- lasso_res %>%
                      tune::show_best("rmse", n = 15) %>%
                      dplyr::arrange(penalty)
top_lasso_models

#default visualization
lasso_res %>% autoplot()

#create a graph to see when there is a significant change in the penalty
lasso_res %>%
  workflowsets::collect_metrics() %>%
  ggplot2::ggplot(aes(penalty, mean, color = .metric)) +
  ggplot2::geom_errorbar(aes(ymin = mean - std_err,
                             ymax = mean + std_err),
                         alpha = 0.5) +
  ggplot2::geom_line(size = 1.5) +
  ggplot2::scale_x_log10()
```

*5. Identify Best Model*
```{r}
#select the lasso model with the lowest rmse
lasso_lowest_rmse <- lasso_res %>%
                        tune::select_best("rmse")

#finalize the workflow by using the selected lasso model
best_lasso_wflow <- lasso_wflow %>%
                      tune::finalize_workflow(lasso_lowest_rmse)
best_lasso_wflow

#one last fit on the training data
best_lasso_fit <- best_lasso_wflow %>%
                    parsnip::fit(data = train_data)

#create a table of model fit
lasso_tibble <- best_lasso_fit %>%
                    workflowsets::extract_fit_parsnip() %>%
                    broom::tidy() %>%
                    dplyr::mutate_if(is.numeric, round, 4)
lasso_tibble
```

*6. Model evaluation*
```{r}
#extract model from final fit
x_lasso <- best_lasso_fit$fit$fit$fit

#plot how number of predictors included in LASSO model changes with the tuning parameter
plot(x_lasso, "lambda")


#find predictions and intervals
lasso_resid <- best_lasso_fit %>%
                  broom.mixed::augment(new_data = train_data) %>%
                  dplyr::select(.pred, BodyTemp) %>%
                  dplyr::mutate(.resid = BodyTemp - .pred)

#plot model predictions from tuned model versus actual outcomes
#now use ggplot
ggplot2::ggplot(lasso_resid, aes(x = BodyTemp, y = .pred)) +
  geom_abline(slope = 1, intercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "LASSO fit: Predicted vs. Actual Body Temperature",
        x = "Actual Body Temperature (F)",
        y = "Predicted Body Temperature (F)")

#plot model with residuals
ggplot2::ggplot(lasso_resid, aes(x = .pred, y = .resid))+
  geom_hline(yintercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "LASSO fit: Residuals vs. Fitted Body Temperature",
        x = "Fitted Body Temperature (F)",
        y = "Residual")

#print model performance
lasso_res %>%
  tune::show_best(n = 10) %>%
  dplyr::select(rmse = mean, std_err, penalty) %>%
  dplyr::mutate(rmse = round(rmse, 3),
                std_err = round(std_err, 4),
                `log penalty` = round(log(penalty), 3),
                .keep = "unused")
#this shows the 10 best performing hyperparameter sets

#compare model performance to null model
lasso_RMSE <- lasso_res %>%
                tune::show_best(n = 1) %>%
                dplyr::transmute(
                  rmse = round(mean, 3),
                  SE = round(std_err, 4),
                  model = "LASSO") %>%
               dplyr::bind_rows(tree_RMSE)
```

<br>

**Random Forest Model**
Most of the code for this section comes from the [TidyModels Tutorial Case Study](https://www.tidymodels.org/start/case-study/).
*1. Model Specification*
```{r}
#define the RF model
rf_mod <-
  parsnip::rand_forest(mtry = tune(),
                       min_n = tune(),
                       trees = 1000) %>%
  parsnip::set_engine("ranger", 
                      num.threads = cores,
                      importance = "permutation") %>%
  parsnip::set_mode("regression")

#use the recipe specified earlier (line 133)

#show what will be tuned
rf_mod %>% tune::parameters()
```

*2. Workflow Definition*
```{r}
#define workflow for RF regression
RF_wflow <- workflows::workflow() %>%
               workflows::add_model(RF_mod) %>%
               workflows::add_recipe(flu_rec)
```

*3. Tuning Grid Specification*
```{r}
#tuning grid specification
RF_grid <- expand.grid(mtry = c(3, 4, 5, 6),
                       min_n = c(40, 50, 60),
                       trees = c(500, 100))
```

*4. Tuning Using Cross-Validation and the `tune_grid()` function*
```{r}
#tune the model with previously specified cross-validation and RMSE as target metric
RF_res <- RF_wflow %>%
              tune::tune_grid(resamples = folds,
                              grid = RF_grid,
                              control = control_grid(verbose = TRUE, save_pred = TRUE),
                              metrics = metric_set(rmse))

#look at top 5 RF models
top_RF_models <- RF_res %>%
                    tune::show_best("rmse", n = 5)
top_RF_models

#default visualization
RF_res %>% autoplot()
```

*5. Identify Best Model*
```{r}
#select the lasso model with the lowest rmse
RF_lowest_rmse <- RF_res %>%
                      tune::select_best("rmse")

#finalize the workflow by using the selected RF model
best_RF_wflow <- RF_wflow %>%
                      tune::finalize_workflow(RF_lowest_rmse)
best_RF_wflow

#one last fit on the training data
best_RF_fit <- best_RF_wflow %>%
                    parsnip::fit(data = train_data)

#create a table of model fit
RF_tibble <- best_RF_fit %>%
                    workflowsets::extract_fit_parsnip() %>%
                    broom::tidy() %>%
                    dplyr::mutate_if(is.numeric, round, 4)
RF_tibble
```

*6. Model evaluation*
```{r}
#extract model from final fit
x_RF <- best_RF_fit$fit$fit$fit

#plot how number of predictors included in LASSO model changes with the tuning parameter
plot(x_lasso, "lambda")


#find predictions and intervals
lasso_resid <- best_lasso_fit %>%
                  broom.mixed::augment(new_data = train_data) %>%
                  dplyr::select(.pred, BodyTemp) %>%
                  dplyr::mutate(.resid = BodyTemp - .pred)

#plot model predictions from tuned model versus actual outcomes
#now use ggplot
ggplot2::ggplot(lasso_resid, aes(x = BodyTemp, y = .pred)) +
  geom_abline(slope = 1, intercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "LASSO fit: Predicted vs. Actual Body Temperature",
        x = "Actual Body Temperature (F)",
        y = "Predicted Body Temperature (F)")

#plot model with residuals
ggplot2::ggplot(lasso_resid, aes(x = .pred, y = .resid))+
  geom_hline(yintercept = 0, color = "red", lty = 2) +
  geom_point() +
  labs(title = "LASSO fit: Residuals vs. Fitted Body Temperature",
        x = "Fitted Body Temperature (F)",
        y = "Residual")

#print model performance
lasso_res %>%
  tune::show_best(n = 10) %>%
  dplyr::select(rmse = mean, std_err, penalty) %>%
  dplyr::mutate(rmse = round(rmse, 3),
                std_err = round(std_err, 4),
                `log penalty` = round(log(penalty), 3),
                .keep = "unused")
#this shows the 10 best performing hyperparameter sets

#compare model performance to null model
lasso_RMSE <- lasso_res %>%
                tune::show_best(n = 1) %>%
                dplyr::transmute(
                  rmse = round(mean, 3),
                  SE = round(std_err, 4),
                  model = "LASSO") %>%
               dplyr::bind_rows(tree_RMSE)
```
